<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

# 机器学习概要

<br>

<br>

<br>

<br>

<center>V 1.0</center>

<div style="page-break-after: always;"></div>

### 第 1 章 数学基础

<div style="page-break-after: always;"></div>

### 第 2 章 机器学习基础

<div style="page-break-after: always;"></div>

## 第一部分 监督学习

### 第 3 章 线性模型

<br>

<br>

<center>如无必要，勿增实体。</center>
<div align="right">—— 奥卡姆剃刀</div>

<br>

<br>

线性模型是机器学习中应用最广泛的模型，在很多情况下，即使线性模型不能直接解决问题，至少也能得到一个不错的逼近。**当你面对一个棘手的问题时，应该首先试试线性方程组。**

> 线性模型如此普遍的深层原因是存在于许多或大部分物理现象中的**平滑性**（“自然不允许跳跃”）。每一个光滑（可微）函数都可以在一个点 $x_c$ 附近得到它的**泰勒展开式逼近**。这个展开式序列的第二项就是线性的，由梯度 $\nabla f(x_c)$ 和位移量的标量积给定，余弦以二阶的速度收敛到零：
> $$
> f(x) = f(x_c) + \nabla f(x_c) \cdot (x-x_c) + O(\|x-x_c\|^2)
> $$
> 因此，在平滑系统中，如果考虑与一个特定点 $x_c$ 相距很近的点，那么线性逼近是一个合理的起点。 

####3.1 线性回归 

输入与输出的线性相关是一个广泛采用的模型，模型中每一项的**权重系数**都为这一项对应的特征的重要性提供了一个直观的解释：某一项权重系数的绝对值越大，对应的属性的影响就越大。

输出与输入参数成线性关系的这一假设可以表示为：
$$
y_i = \boldsymbol{w}^\top\boldsymbol{x}_i + \epsilon_i \tag{3-1}
$$
其中 $\boldsymbol{w} = (w_1,...,w_d)$ 为待确定的**权重向量**，$\epsilon_i$ 是误差项。在大多数情况下，假设误差项 $\epsilon_i$ 服从高斯分布。即使一个线性模型能正确地解释这些现象，误差仍然会在测量时产生。

现在我们寻找一个权重向量 $\boldsymbol{w}$，使得线性函数
$$
\hat{f}(\boldsymbol{x}) = \boldsymbol{w}^\top\boldsymbol{x} \tag{3-2}
$$
尽可能逼近实验数据。这一目标可以通过寻找使平方误差和最小的 $\boldsymbol{w}^*$ 来达到（**最小二乘**逼近）：
$$
\text{Error}(\boldsymbol{w}) = \sum_{i=1}^l(\boldsymbol{w}^\top\boldsymbol{x}_i - y_i)^2 \tag{3-3}
$$
在一个零测量误差和完美线性模型的不现实场景里，人们只需要解一系列**线性方程** $\boldsymbol{w}^\top\boldsymbol{x}_i = y_i$，每一个这样的方程对应于一次测量。如果这个系统是良好定义的（$d$ 个非冗余方程对应 $d$ 个未知数），我们就可以通过对系数矩阵求逆来解这些方程。

在实际操作中，让模型误差 $\text{Error}$ 达到零是不可能的，另外数据点 $(\boldsymbol{x}_i,y_i)$ 的个数会远远大于参数个数 $d$。因此，我们需要**通过允许误差的存在来将线性方程组的解进行一般化**，从而一般化矩阵的逆。

使误差函数 $\text{Error}$ 最小化很简单：计算梯度，然后要求其为零。我们首先将误差函数 $\text{Error}$ 写成矩阵形式，矩阵 $\boldsymbol{X}$ 是以向量 $\boldsymbol{x}_i$ 为行的矩阵，$\boldsymbol{y} = (y_1,...,y_l)$，为了求导方便在前面乘以 $\frac{1}{2}$ 记为 $J(\boldsymbol{w})$:
$$
J(\boldsymbol{w}) = \frac{1}{2}\sum_{i=1}^l(\boldsymbol{w}^\top\boldsymbol{x}_i-y_i)^2=\frac{1}{2}\Big[(\boldsymbol{X}\boldsymbol{w}-\boldsymbol{y})^\top(\boldsymbol{X}\boldsymbol{w}-\boldsymbol{y})\Big] \tag{3-4}
$$
计算梯度：
$$
\nabla_{\boldsymbol{w}}J(\boldsymbol{w}) = \boldsymbol{X}^\top\boldsymbol{X}\boldsymbol{w} - \boldsymbol{X}^\top\boldsymbol{y} \tag{3-5}
$$
我们令梯度为零可以得到 $\boldsymbol{w}$ 的最优值：
$$
\boldsymbol{w}^* = (\boldsymbol{X}^\top\boldsymbol{X})^{-1}\boldsymbol{X}^\top\boldsymbol{y} \tag{3-6}
$$
矩阵 $(\boldsymbol{X}^\top\boldsymbol{X})^{-1}\boldsymbol{X}^\top$ 称为**广义逆矩阵**，它是对那些非方阵矩阵的矩阵求逆的一种很自然的延伸。如果矩阵是可逆的，并且这个问题可以零误差地求解，那么广义逆矩阵就等于逆矩阵。但是一般情况下，例如训练实例数大于权重系数的个数时，寻求一个最小二乘解能避免无法找到精确解的尴尬，并且能提供一个统计上有意义的“折中”解。

解等式 $\text{(3-6)}$ 的方法是“一招制胜”：从实验数据中算出广义逆矩阵，然后相乘得到最优权重系数。在某些情况下，如果训练实例数非常大，**基于迭代方法的梯度下降**就会更受欢迎：从一个初始权重系数开始，然后沿着负梯度的方向小步移动，直到梯度变为零，到达一个稳定点。

具体而言，梯度下降每一次迭代时建议新的点为：
$$
\boldsymbol{w}' = \boldsymbol{w} - \epsilon \nabla_{\boldsymbol{w}} J(\boldsymbol{w})
$$
其中，$\epsilon$ 为学习率，确定步长的大小。实际操作中，我们在梯度 $\nabla_{\boldsymbol{w}}J(\boldsymbol{w})$ 接近零时停止迭代。算法首先将步长 $\epsilon$ 和容差 $\delta$ 设为小的正数，然后 

while  $$ 

当训练实例数很大的时候，式 $\text{(3-6)}$ 是超定情况的线性系统的解（线性方程多于变量），特别是矩阵 $\boldsymbol{X}^\top\boldsymbol{X}$ 必须是非奇异的。 在很多情况下，即使 $\boldsymbol{X}^\top\boldsymbol{X}$ 是可逆的，也有可能因为训练点集的分布不那么合适而导致不稳定。

> **稳定性**是指样本点中的微扰只会造成结果中的微小改变。

如果没有办法来改变训练样本点的选择，而样本点又没有如愿地分布时，用以保证数值稳定性的数学工具是**岭回归**，它在需要最小化的（最小二乘） 误差函数中加入了一个正则化项：
$$
\text{error}(\boldsymbol{w};\lambda) = \sum_{i=1}^l(\boldsymbol{w}^\top\boldsymbol{x}_i-y_i)^2 + \lambda\boldsymbol{w}^\top\boldsymbol{w} \tag{3-7}
$$
对 $\boldsymbol{w}$ 进行最小化，得到：
$$
\boldsymbol{w}^* = (\lambda\boldsymbol{I} + \boldsymbol{X}^\top\boldsymbol{X})^{-1}\boldsymbol{X}^\top\boldsymbol{y} \tag{3-8}
$$
在对角线上插入这些小的项使得求逆变得更加具有健壮性。事实上，我们通常也会将权重向量的规模列入考虑范围，以避免出现陡峭的差值平面。可以想象，较大的 $\lambda$ 值会导致总权重的收缩。

> 这一方法的理论基础是 Tichonov 正则化，它是处理众多不适定问题的最通用的方法。通过同时最小化实验误差和惩罚项，使得模型不仅能很好地拟合，并且还足够简单，避免在估计复杂模型时出现大的变化。

#### 3.2 处理非线性函数关系的技巧

 在很多情况下，函数 $f(\boldsymbol{x}) = \boldsymbol{w}^\top\boldsymbol{x}$ 是不实用的，因为它存在很多限制。例如它假设 $f(0)=0$ ，这可以通过加入一个常数项 $w_0$ 从线性模型变成仿射模型来解决：$f(\boldsymbol{x}) = w_0 + \boldsymbol{w}^\top\boldsymbol{x}$。 这一常数项可以并入到内积中，只需要重新定义 $\boldsymbol{x} = (1,x_1,...,x_d)$，这样式 $\text{(3-2)}$ 对仿射模型依然成立。

然而其他部分还属于最小二乘逼近的简单情况。这可以用一个技巧来解决：仍然用线性模型，只不过将原输入数据 $\boldsymbol{x}$ 进行非线性转换得到**非线性属性**，并在其上应用线性模型。

可以定义这样一个函数集：
$$
\phi_1,...,\phi_n:\mathbb{R}^d \rightarrow\mathbb{R}^n
$$
它从输入空间映射到某个更为复杂的空间，使得我们可以用向量 $\boldsymbol{\varphi}(\boldsymbol{x}) = (\phi_1(\boldsymbol{x}),...,\phi_n(\boldsymbol{x}))$ 进行线性回归，而不是原始数据 $\boldsymbol{x}$。 

> 例如，如果 $d=2$ 并且输入向量 $\boldsymbol{x} = (x_1,x_2)$，输出的二次相关可以通过如下的**基函数**得到：
> $$
> \phi_1(\boldsymbol{x}) = 1, \quad \phi_2(\boldsymbol{x}) = x_1, \quad \phi_3(\boldsymbol{x}) = x_2, \\
> \phi_4(\boldsymbol{x}) = x_1x_2, \quad \phi_5(\boldsymbol{x}) = x_1^2, \quad \phi_6(\boldsymbol{x}) = x_2^2
> $$
> 这里定义 $\phi_1(\boldsymbol{x})$ 是为了使函数中允许常数项的存在。线性回归方法可以用在经过这些基函数变换后的六维向量上，而不是原来的二维参数向量。

更精确地说，我们寻找如下的一个函数，它是权重向量 $\boldsymbol{w}$ 和属性向量 $\boldsymbol{\varphi}(\boldsymbol{x})$ 的标量积：
$$
\hat{f}(\boldsymbol{x}) = \boldsymbol{w}^\top\boldsymbol{\varphi}(\boldsymbol{x}) \tag{3-9}
$$
输出是这些变换后的属性的加权和。其求解方法与线性回归非常类似，也可以通过最小二乘法来计算。令 $\boldsymbol{x}'_i = \boldsymbol{\varphi}(\boldsymbol{x}_i),i=1,...,l$ 表示训练输入元组 $\boldsymbol{x}_i$ 的变形。如果 $\boldsymbol{X}'$ 是以 $\boldsymbol{x}_i$ 为行向量的矩阵，那么关于最小二乘逼近的最优权重系数可以这样求得：
$$
\boldsymbol{w}^* = (\boldsymbol{X}'^\top\boldsymbol{X}')^{-1}\boldsymbol{X}'^\top\boldsymbol{y} \tag{3-10}
$$

#### 3.3 最小二乘法的统计学解释

**卡方**检验函数被广泛应用于估计拟合优度，用以表示真实值与模型估计值之问的偏离程度：
$$
\chi^2 = \sum_{i=1}^N \bigg(\frac{y_i - f(x_i)}{\sigma_i}\bigg)^2 \tag{3-11}
$$
如果参数 $\sigma_i$ 都等于 1，那么 $\chi^2$ 测量真实值 $y_i$ 和模型估计值 $f(x_i)$ 之间的平方误差和，也就是 3.1 节中描述的 $\text{Error}$。但是在某些情况下，不同数据点的测量过程可能是不同的，人们对于某个测量的误差 $\sigma_i$ 有一个估计，假设为标准差。卡方的定义就是当计算 $\chi^2$ 时，误差必须除以标准差 $\sigma_i$ 进行归一化，这样得到的结果是与实际误差规模无关的一个数，并且它的含义是经过了标准化的。

回顾最小二乘拟合的过程：

1.  假设大自然和实验程序会产生独立的实验样本 $(x_i,y_i)$。假设 $y_i$ 的测量值受到了误差的影响，这个误差服从正态（高斯）分布。
2. 如果模型参数 $\boldsymbol{c}$ 是已知的，那么就可以估计我们测量数据的概率，这在统计学中称为数据的**似然率**。 
3. 最小二乘法拟合所找到的就是使得我们数据的似然率最大化的参数。最小二乘是一种**最大似然估计**，使得选择的模型和观察到的数据之间的“契合度”最大化。

高斯分布中，对于单个数据点，它的位置与测量值 $y_i$ 的距离在区间 $\mathbb{d}y$ 中的概率正比于：
$$
\exp\Bigg(-\frac{1}{2}\bigg(\frac{y_i-f(x_i,\boldsymbol{c})}{\sigma_i}\bigg)^2\Bigg)\mathbb{d}y \tag{3-12}
$$
由于数据点是独立生成的，整个实验序列（似然性）的概率是单个概率的乘积：
$$
\mathbb{d}P \propto \prod_{i=1}^N \exp\Bigg(-\frac{1}{2}\bigg(\frac{y_i-f(x_i,\boldsymbol{c})}{\sigma_i}\bigg)^2\Bigg)\mathbb{d}y \tag{3-13}
$$
由于我们是求关于 $\boldsymbol{c}$ 的最大值，常数因子（像 $(\mathbb{d}y)^N$）是可以略去的。最大化这个似然率等价于最大化它的对数，当常数项被略去的时候，式 $\text{(3-13)}$ 的对数就正好是式 $\text{(3-11)}$ 中的卡方的定义。因此，最小二乘拟合事实上就是最大似然估计。 

#### 3.4 线性模型分类器 

假设输出变量是二值的（如 $\pm1$），此时线性模型可以用作判别器，基本思路是让一个垂直于向量 $\boldsymbol{w}$ 的超平面将这两类隔离开。

> 平面是直线的一般化；同样，当维度大于 $3$ 时，超平面就是平面的一般化。 

训练过程的目标是找到最佳的超平面，使得属于不同类别的实例分别位于这个超平面的两边。用数学语言描述就是，要找到最佳的系数向量 $\boldsymbol{w}$ 使得决策程序
$$
y=\begin{cases}
+1\quad\text{如果 }\boldsymbol{w}^\top\boldsymbol{x}\ge 0 \\
-1\quad\text{其他情况}
\end{cases} \tag{3-14}
$$
表现得最好。决定**最优线性分离函数**（几何上的一个超平面）的方法取决于分类标准和误差度量的选择。

如果要进行回归，可以要求第一类的点映射到 $+1$，第二类的点映射到 $-1$。这是一个比可分离更强的要求，但是让我们能够使用回归方法，像梯度下降法和广义逆矩阵法。此外，最小二乘法不仅可以实现两个类别样本的分类（如果这两类样本是线性可分的），还可以让分类是健壮的，分割超平面离样本都很远。

> 通过强制模型的输出为 $+1$ 或 $-1$，加上平方误差惩罚项，可以避免得到过多的分割超平面，从而提升模型的稳定性。 

如果训练实例线性不可分，要么忍受一些训练误差，要么尝试使用 3.2 节中的技巧，从原始数据中计算出一些非线性的属性，使得变化后的输入能够被分离。

<div style="page-break-after: always;"></div>

### 第 4 章 非线性模型

<br>

<br>

<center>想学会飞翔，就必须先学会站立、行走、奔跑、攀登和跳舞，没有人能直接飞翔。</center> 
<div align="right">—— 尼采</div>

<br>

<br>

让我们继续沿着从线性模型到非线性模型的道路进行探索，先从线性模型的逐渐修改开始，一步一步接触到非线性模型的精髓。

#### 4.1 logistic 回归

在统计学中，logistic 回归被用于预测分类变量的各种结果的概率。这个名字有一定的误导性，事实上 logistic 回归是一种分类技术，而不是回归。但这种分类的通过概率的估计得到的，因此使用术语“回归”。

logistic 回归大部分是由一个线性模型运行的，但 **logistic 函数** (图 4-1) 被用来将线性预测器的输出转化为一个 $0\sim1$ 的值，这也可以解释为概率。

![4-1](4-1.jpeg)

<center>图 4-1 logistic 函数把输入值以平稳的方式转换为 0~1 的输出值</center><br>

标准的 logistic 函数由下式定义：
$$
P(t) = \frac{1}{1+e^{-t}} \tag{4-1}
$$
其中 $e$ 是欧拉数。变量 $t$ 可以是时间，不过这里 $t$ 是线性模型的输出，回顾式 $\text{(3-2)}$ 有：
$$
y = \frac{1}{1+e^{-(\boldsymbol{w}^\top\boldsymbol{x})}} \tag{4-2}
$$
这里线性模型 $\boldsymbol{w}$ 也可以包括一个常数值 $w_0$，前提是在输入值列表中人为添加一个总是等于 1 的输入值 $x_0$。 式 $\text{(4-2)}$ 可变化为： 
$$
\ln\frac{y}{1-y} = \boldsymbol{w}^\top\boldsymbol{x} \tag{4-3}
$$
因为我们将输出值 $y$ 看作是正例的概率，所以 $1-y$ 是其反例的概率，两者的比值：
$$
\frac{y}{1-y} \tag{4-4}
$$
称为**几率**，反映了 $\boldsymbol{x}$ 作为正例的相对可能性。因此式 $\text{(4-2)}$ 实际上是在用线性回归模型的预测结果去逼近真实标记的**对数几率**，因此 logistic 回归又被称为对数几率回归。

线性变换权重的最佳值是通过最大似然估计来确定的。给定数据集 $\{(\boldsymbol{x}_i,y_i)\}_{i=1}^l$，logistic 回归模型最大化对数似然：
$$
\text{LogLikelihood}(\boldsymbol{w}) = \sum_{i=1}^l \ln p(y_i|\boldsymbol{x}_i;\boldsymbol{w}) \tag{4-5}
$$
即令每个样本属于其真实标记的概率越大越好。令 $y_i$ 为所观察到的输出（1 或 0），对应输入为 $\boldsymbol{x}_i$。若正确分类为 1，则获得测量输出值 $y_i$ 的概率为 $p(y=1|\boldsymbol{x}_i)$；若正确分类标签为 0，则 $p(y=0|\boldsymbol{x}_i)=1-p(y=1|\boldsymbol{x}_i)$。于是式 $\text{(4-5)}​$ 可以重写为：
$$
\text{LogLikelihood}(\boldsymbol{w}) = \sum_{i=1}^l\{y_i\ln p(y=1|\boldsymbol{x}_i,\boldsymbol{w}) + (1-y_i)\ln(1-p(y=1|\boldsymbol{x}_i,\boldsymbol{w}))\} \tag{4-6}
$$
似然率与系数（权重）$\boldsymbol{w}$ 的相关性已经非常明确了。 

因为 logistic 回归的输出就是正例的概率，显然有：
$$
\begin{align}
p(y=1|\boldsymbol{x}_i,\boldsymbol{w}) &= \frac{1}{1+e^{-(\boldsymbol{w}^\top\boldsymbol{x})}} = \frac{e^{\boldsymbol{w}^\top\boldsymbol{x}}}{1+e^{\boldsymbol{w}^\top\boldsymbol{x}}} \\
p(y=0|\boldsymbol{x}_i,\boldsymbol{w}) &= \frac{1}{1+e^{\boldsymbol{w}^\top\boldsymbol{x}}}
\end{align} \tag{4-7}
$$
因此最大化式 $\text{(4-6)}$ 等价于最小化
$$
L(\boldsymbol{w}) = \sum_{i=1}^l\Big(-y_i\boldsymbol{w}^\top\boldsymbol{x}_i + \ln(1+e^{\boldsymbol{w}^\top\boldsymbol{x}_i})\Big) \tag{4-8}
$$
由于上述表达式中的非线性性，我们不可能找到使似然函数最大化的权重的解析表达式，而必须用迭代过程来代替，例如梯度下降法。

#### 4.2 局部加权回归

本节中考虑的方法类似于最近邻值的输出的线性组合，但我们不会只关注 $K$ 个最近邻值而消除所有其他值的影响。这是一种平滑的变化：根据和被预测的实例之间的距离来逐渐减少实例对预测的影响，而不是选择一组 $K$ 个胜者。 

**局部加权回归**是一种懒惰的基于存储的技术，这意味着所有点和评估值都被存储了，而只有查询特定的某点的时候才会基于请求建立特定的模型。

为了预测一个点 $\boldsymbol{q}$（称为查询点）的评估结果，我们对训练点应用线性回归。为了确保在确定回归参数过程中的局部性（相近的点更相关），给每个样本点分配一个权重，这个权重会随着与查询点距离的增加而减小，我们用 $s_i$ 来表示。

加权后的最小二乘拟合的目标是最小化下面的加权误差（式 $\text{(3-3)}$ 中隐式地假设了每个点的权重是一样的）：
$$
\text{error}(\boldsymbol{w};s_1,...,s_n) = \sum_{i=1}^l s_i(\boldsymbol{w}^\top\boldsymbol{x}_i-y_i)^2 \tag{4-9}
$$
为了最小化式 $\text{(4-7)}$，可以令其关于 $\boldsymbol{w}$ 的梯度等于 0，得到如下解：
$$
\boldsymbol{w}^* = (\boldsymbol{X}^\top\boldsymbol{S}^2\boldsymbol{X})^{-1}\boldsymbol{X}^\top\boldsymbol{S}^2\boldsymbol{y} \tag{4-10}
$$
其中 $\boldsymbol{S} = \text{diag}(s_1,...,s_d)$。当所有权重相等时，式 $\text{(4-8)}$ 简化为式 $\text{(3-6)}$。 

根据存储的样本到查询点的距离，可以使用以下函数来描述它们的重要性：
$$
s_i = \exp\bigg(-\frac{\|\boldsymbol{x}_i-\boldsymbol{q}\|^2}{W_K}\bigg) \tag{4-11}
$$
其中 $W_K$ 是度量“核宽度”的一个参数，即对远距离实例的灵敏度；当距离远大于 $W_K$ 时，重要性迅速衰减至 0。 

#### 4.3 用 LASSO 来缩小系数和选择输入值

考虑线性回归模型时，岭回归是一种通过二次方式来惩罚大系数，从而使得模型更稳定的方法，如式 $\text{(3-7)}$ 所示。

普通的最小二乘估计法通常偏差较小，但是方差较大。为了提高准确率，有时可以将一些系数缩小或者设置为零。通过这样做，我们牺牲一点偏差，以减少预测值的方差，从而可以提高整体的预测准确率。

特征子集选择和岭回归，这两个改进估计的标准技术仍然存在一些缺陷：**子集选择**提供了便于解释的模型，但由于它是一个离散过程，输入变量要么保留，要么删除，该模型的变化也可能是特别大的；**岭回归**是一种连续让系数缩小，从而使得模型更稳定的过程，然而它没有设置任何系数为零，所以无法得出一个易于解释的模型。

![4-2](4-2.jpg)

<center>图 4-2 在 LASSO 中，最好的解决方法出现在二次误差函数等高线接触正方形处，有时会在正方形的角上，对应某些零系数。相反，岭回归的二次约束没有角来让等高线接触，因此权重中很少会产生零</center><br>

最小绝对收缩和选择算符 LASSO 使得一些系数缩小而另一些设置为零，因此保持了子集选择和岭回归两种方法的优势。LASSO 使用**权重绝对值**的总和作为约束 $\|\boldsymbol{w}\|_1$（参数向量的 $L_1$ 范数）。**LASSO 在系数绝对值总和小于一个常数的约束下，使得残差平方和最小化。**通过一个标准技巧，将带约束的优化问题通过拉格朗日乘数法转化为无约束的问题，这相当于将 $\lambda\|\boldsymbol{w}\|_1$ 加入无约束最小化的最小二乘：
$$
\text{LASSOerror}(\boldsymbol{w};\lambda) = \sum_{i=1}^l(\boldsymbol{w}^\top\boldsymbol{x}_i-y_i)^2+\lambda\sum_{j=0}^d|w_j| \tag{4-12}
$$
LASSO 和岭回归一个最主要的区别是，在岭回归中随着惩罚的增加，所有系数减小，但保持非零的状态，而 LASSO 随着惩罚的增加会导致更多的系数变为零。对应的权重为零的输入值就可以消除，从而导致模型使用较少的输入值（输入的**稀疏化**），因此更便于解释。换言之，作为模型构建过程的一部分，LASSO 是一种进行特征选择的嵌入式方法。

> 注意，式 $\text{(4-12)}$ 中 $\lambda\sum_{j=0}^d|w_j|$，当权重为零时不存在导数（偏导数从对应负值的 $-1$ 跳到对应正值的 $+1$），因此无法通过计算导数并令其等于零来求解。优化 LASSO 的问题可以通过引入带线性不等式约束的**二次规划**或更一般的凸优化方法来解决。$\lambda$ 的最佳值可以通过交叉验证来获得。 

> **拉格朗日乘数优化约束问题**
>
> LASSO 将带约束的优化问题转化为无约束的优化问题，并已被广泛应用。在数学优化中，**拉格朗日乘数法**是在带约束的前提下用来寻找函数局部最大值和最小值的方法。带约束问题是通过将各个约束乘以一个参数（一个拉格朗日乘数）转化为无约束的。

<div style="page-break-after: always;"></div>

### 第 5 章 神经网络：多层感知器

<br>

<br>

<center>大自然是大师中的大师，除非从他那里获得灵感，否则其他的都是徒劳无益。</center> 
<div align="right">—— 达·芬奇</div>

<br>

<br>

单一的神经元是一个简单的计算单元，它计算一个标量积，接着是一个 S 型函数。人类的大脑证明，许多简单的单位连接起来的系统，可以产生令人难以置信的智能活动。

#### 5.1 多层感知器

4.1 节中的 logistic 回归模型，通过将 S 型传递函数应用到无限制的线性模型输出上，使得输出可以被解释为一个概率值，它是添加“最小限度的非线性”的简单方法。

> logistic 回归与没有隐藏层却有单一输出值的 MLP 网络有着相同的体系结构，改变的只是进行优化的函数：MLP 优化误差平方和，logistic 回归优化对数似然函数 (LogLikelihood)。

多层感知器神经网络是由大量的高度互联单元（神经元）构成的，它们平行工作，用于解决特定的问题。神经元以层的方式组织起来，之间有前馈信息流（无回路），如图 4-3 所示。

![绘图1 2](4-3.jpg)

<center>图 4-3 单隐藏层的多层感知器：隐藏层的 S 型传递函数引入非线性</center><br>

对于每个层，每个单元首先计算权向量与前一层的输出给出的向量之间的标量积，得到的结果经过一个传递函数，产生下一层的输入。常用的光滑渐近传递函数是 S 型函数（图形形状像字母 S），之前遇见过的 logistic 转换就是一个例子（见图 4-1）。

虽然感知器的建模能力有限，只能用于输入能被输入空间中的一个超平面分开的分类问题，但是 MLP 却是一种通用逼近：如果有足够多的隐藏节点，拥有一个隐藏层的 MLP 能够以任意精度逼近任何光滑函数。

#### 5.2 通过反向传播法学习

跟往常一样，选择一个“向导”函数进行优化。例如光滑的（可导）平方误差和，并使用**梯度下降法**，迭代地计算函数关于权重系数的梯度，并且如果梯度不是 0 就朝着负梯度的方向移动一小步，使得函数值减小。

我们使用微积分中的**链式法则**来计算偏导数，以求得两个或更多函数的复合函数的导数。如果 $f$ 和 $g$ 分别是一个函数，然后链式法则指明如何从 $f$ 的 $g$ 的导数计算出复合函数 $f \circ g$ 的导数。例如 $f\circ g$ 的链式法则是：
$$
\frac{\text{d}f}{\text{d}x} = \frac{\text{d}f}{\text{d}g}\cdot\frac{\text{d}g}{\text{d}x}
$$
MLP 中的基本函数是：标量积，然后 S 型函数，接下来又是标量积，如此反复一直到输出层，计算误差。对于 MLP 网络，其梯度可以被高效地计算，它的计算需要的操作数正比于权重系数的数量，实际计算中所用的简单公式与前向传递（从输入到输出）类似，只是方向相反，从输出误差到输入，因此称之为误差的反向传播。目前，神经网络中的一个流行的技术就是通过误差的反向传播进行学习：计算梯度，然后沿着负梯度的方向小步移动。

> 梯度下降法只能达到局部最优点（梯度为 0），并不能保证是全局最优解。ML 的目标是泛化，对于这一目标，全局最优并非必要，甚至导致过度训练。

考虑标准的多层感知器，只有相邻的层之间有权重系数，差平方和能量函数定义为：
$$
E(\boldsymbol{w}) = \frac{1}{2}\sum_{p=1}^P E_p = \frac{1}{2}\sum_{p=1}^P (t_p-o_p)^2 \tag{5-1}
$$
其中 $t_p$ 和 $o_p$ 分别是目标值和当前 MLP 的输出值。S 型传递函数是 $f(x) = \frac{1}{1+e^{-x}}$。 

> $f(x) = \frac{1}{1+e^{-x}}$ 被称为 sigmoid 函数或 logistic 函数，它的导数很容易用它的输出表示：
> $$
> \frac{\text{d}f(x)}{\text{d}x} = f(x)\cdot(1-f(x))
> $$
> 因此在梯度下降法中计算导数会很方便，这也是经常采用 sigmoid 函数作为 S 型传递函数的原因。

**批量梯度下降**是梯度下降法的一个教科书版本。在得到了所有样本误差的梯度后，记为 $g_k = \nabla E(\boldsymbol{w}_k)$，下一次迭代 $k+1$ 的权重系数被下面的式子更新为：
$$
\boldsymbol{w}_{k+1} = \boldsymbol{w}_k - \epsilon g_k \tag{5-2}
$$
其中 $\epsilon$ 是表示学习率，其作用是控制每一步调整权的程度。 学习速率不应该太小，避免学习时间过长（每次迭代权重系数的改变都很小），学习速率也不应该太大，避免震荡导致的能量函数疯长（只有在步子很小时，沿着梯度方向的改变才能保证函数值减小）。

由于能量函数 $E$ 是许多项的和，每一项对应一个样本，因而梯度是相应的局部梯度 $\nabla E_p(\boldsymbol{w}_k)$ 的和，$\nabla E_p(\boldsymbol{w}_k)$ 是第 $p$ 个样本中误差的梯度 $(t_p-o_p)^2$。 

如果我们有上百万训练实例，首先对 $\nabla E_p(\boldsymbol{w}_k)$ 进行求和，然后走一小步。于是，马上会想到：在计算一个 $\nabla E_p(\boldsymbol{w}_k)$ 后立即沿着负梯度的方向走一小步会如何呢？如果这一步非常小，得到的权重与初始的差别将很小，并且接下来的梯度 $\nabla E_p(\boldsymbol{w}_{k+j})$ 将非常类似于原始的那些 $\nabla E_p(\boldsymbol{w}_k)$。 

如果以随机的顺序选择样本，就得到了**随机梯度下降**。

>生物神经元不是很擅长复杂和长期的计算，所以随机梯度下降具有多种生物学意义。例如孩子学习识别数字，当他犯了一个错时，应该立即纠正，而不是等收集了成百上千的错误之后再纠正。

随机梯度下降的更新由下式给出：
$$
\boldsymbol{w}_{k+1} = \boldsymbol{w}_k - \epsilon\nabla E_p(\boldsymbol{w}_k) \tag{5-3}
$$
其中样本 $p$ 是每次迭代时从训练集中随机选择的，$\epsilon$ 是学习速率。相对于批量随机梯度下降，也就是 $E$ 的完整梯度被用作搜索方向，这种方法的优势在于，局部梯度 $\nabla E_p(\boldsymbol{w}_k)$ 只需要单一的向前和向后传递，因此该方法的不精确性可以通过单次迭代所需的较低计算量进行补偿。 

> 小批量反向传播是一个折中的选项，仅仅在一个随即大小为 $B$ 的样本子集（批）上运行向前和向后传播来积累部分梯度。每 $B$ 个向前传播修改权重。极端情况是，$B$ 等于 1 时相当于随机梯度下降，$B$ 等于样本的总数量时相当于批量梯度下降。 

#### 5.3 随机梯度下降的推导

现在我们考虑更一般的情况，MLP 网络包含多个输出单元。我们首先重新定义误差 $E$：
$$
E(\boldsymbol{w}) = \frac{1}{2}\sum_{p=1}^P\sum_{k=1}^K(t_{kp}-o_{kp})^2 \tag{5-4}
$$
其中 $t_{kp}$ 和 $o_{kp}$ 分别是训练样例 $p$ 第 $k$ 个输出单元对应的目标值和输出值。

随机梯度下降迭代地处理训练样例，每次处理一个。对于每个训练样例 $p$，利用这个样例对应误差的梯度$\nabla E_p(\boldsymbol{w})$ 修改权值。其中，$E_p$ 是训练样例 $p$ 的误差，通过对网络中所有输出单元的求和得到：
$$
E_p(\boldsymbol{w}) = \frac{1}{2}\sum_{k=1}^K(t_k-o_k)^2 \tag{5-5}
$$
我们增加一个下标 $j$ 来表示网络中的第 $j$ 个单元。

- $x_{ji}$：单元 $j$ 的第 $i$ 个输入
- $w_{ji}$：与单元 $j$ 的第 $i$ 个输入相关联的权值
- $net_j=\sum_i w_{ji}x_{ji}$：单元 $j$ 的输入的加权和
- $o_j$：单元 $j$ 计算出的输出
- $t_j$：单元 j 的目标输出
- $f$：sigmoid 函数
- $Downstream(j)$：单元的直接输入中包含单元 $j$ 的输出的单元的集合 

现在我们导出 $\nabla E_p(\boldsymbol{w})$ 的表达式，实现公式 $\text{(5-3)}$ 中所讲的随机梯度下降法则。 因为权值 $w_{ji}$ 仅能通过 $net_j$ 影响网络的其他部分，所以我们可以使用链式法则得到：
$$
\begin{align}
\frac{\partial E_p}{\partial w_{ji}} &= \frac{\partial E_p}{\partial net_j}\frac{\partial net_j}{\partial w_{ji}} \\
&=\frac{\partial E_p}{\partial net_j} x_{ji}
\end{align}\tag{5-6}
$$
于是，梯度下降的更新（式 $\text{(5-3)}$）可以重写为：
$$
{w_{ji}}_{k+1} = {w_{ji}}_k - \epsilon\frac{\partial E_p}{\partial net_j} x_{ji} \tag{5-7}
$$
下面我们剩下的任务就是为 $\frac{\partial E_p}{\partial net_j}$ 导出一个表达式，依次考虑两种情况：一种情况是单元 $j$ 是网络的一个输出单元， 另一种情况是 $j$ 是一个内部单元。 

**情况 1：输出单元的权值训练法则** 

就像 $w_{ji}$ 仅能通过 $net_j$ 影响网络一样，$net_j$ 仅能通过 $o_j$ 影响网络。我们再次使用链式法则：
$$
\begin{align}
\frac{\partial E_p}{\partial net_j} &= \frac{\partial E_p}{\partial o_j}\frac{\partial o_j}{\partial net_j} \\
&=\bigg(\frac{\partial}{\partial o_j}\frac{1}{2}\sum_{k=1}^K(t_k-o_k)^2\bigg)\bigg(\frac{\partial f(net_j)}{\partial net_j}\bigg) \\
&=\bigg(\frac{\partial}{\partial o_j}\frac{1}{2}(t_j-o_j)^2\bigg)f(net_j)(1-f(net_j))\\
&= \bigg(\frac{1}{2}2(t_j-o_j)\frac{\partial(t_j-o_j)}{\partial o_j}\bigg)o_j(1-o_j)\\
&=-(t_j-o_j)o_j(1-o_j)
\end{align}\tag{5-8}
$$
然后代入式 $\text{(5-7)}$，我们便推导出了输出单元的随机梯度下降法则：
$$
{w_{ji}}_{k+1}={w_{ji}}_k + \epsilon(t_j-o_j)o_j(1-o_j)x_{ji} \tag{5-9}
$$
**情况 2：隐藏单元的权值训练法则**

对于网络中的隐藏单元，推导 $w_{ji}$ 必须考虑 $w_{ji}$ 间接地影响网络输出，从而影响 $E_p$。因此我们定义网络中单元 $j$ 的所有直接下游单元的集合，用 $Downstream(j)$ 表示，$net_j$ 只能通过 $Downstream(j)$ 中的单元影响网络输出（在影响 $E_p$）。我们用 $\delta_i$ 来表示任意单元 $i$ 的 $-\frac{\partial E_p}{\partial net_i}$，可以得出如下推导：
$$
\begin{align}
\frac{\partial E_p}{\partial net_j} &= \sum_{k \in Downstream(j)}\frac{\partial E_p}{\partial net_k}\frac{\partial net_k}{\partial net_j}\\
&=\sum_{k\in Downstream(j)}-\delta_k \frac{\partial net_k}{\partial net_j} \\
&=\sum_{k \in Downstream(j)}-\delta_k\frac{\partial net_k}{\partial o_j}\frac{\partial o_j}{\partial net_j}\\
&=\sum_{k \in Downstream(j)}-\delta_k w_{kj}\frac{\partial o_j}{\partial net_j}\\
&=\sum_{k \in Downstream(j)}-\delta_k w_{kj} o_j(1-o_j)\\
&=-o_j(1-o_j)\sum_{k \in Downstream(j)}\delta_k w_{kj}
\end{align} \tag{5-10}
$$
然后代入式 $\text{(5-7)}$，我们便推导出了隐藏单元的随机梯度下降法则：
$$
{w_{ji}}_{k+1}={w_{ji}}_k + \epsilon \Bigg( o_j(1-o_j)\sum_{k \in Downstream(j)}\delta_k w_{kj}\Bigg)x_{ji} \tag{5-11}
$$

<div style="page-break-after: always;"></div>

### 第 6 章 支持向量机

<br>

<br>

<center>科学或许就是系统简化的艺术。</center> 
<div align="right">—— 卡尔·波普尔</div>

<br>

<br>

支持向量机是线性判别方法的集大成者，它为了加强模型的泛化能力，往线性判别方法中加入了额外的优化目标。支持向量机的基本模型就是定义在特征空间上的间隔最大的线性分类器，其中支持向量就是那些处于安全间隔两侧边缘的点。间隔最大使它有别于感知机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。

为了得到最大间隔线性分类器，可以使用标准的**二次规划**，它可以在一定规模下解决此类优化问题。二次规划问题就是目标函数为二次函数、约束条件为线性的最优化问题。如果训练样本不是线性可分的，那么 SVM 需要先对原始样本点做**非线性的变换** $\phi$，从而将其变成（近似）线性可分的。可以将 $\phi$ 看作是一个合适的特征生成函数，它使得变换之后的两类样本点 $\phi(\boldsymbol{x})$ 是线性可分的。

为了找到合适的 $\phi$，还需要重新做特征提取和特征工程。在使用 $\phi$ 变换输入样本后，**SVM 的特征就是要识别的样本和训练样本之间所有的相似性值**，实际上只有**支持向量**才提供非零的贡献。

> SVM 关键的一步就是通过一些交叉验证的方式，人工确定最利于学习和泛化的相似度度量函数，其中就涉及**核函数**的选择。

SVM 可以看作解决了两个问题：一方面，它找到了一个衡量输入向量之间相关性的合适方式，即核函数 $K(\boldsymbol{x},\boldsymbol{y})$；另一方面，它构建了一个线性结构，该线性结构结合了训练样本的输出和新的测试样本，训练样本的输出用相似度来衡量。越相似的输入样本对输出的贡献越大，可以用类似下面的式子来描述：
$$
\sum_{i=1}^{l}\alpha_iy_i K(\boldsymbol{x},\boldsymbol{x}_i)
$$
其中 $l$ 是训练样本的数量，$y_i$ 是训练样本 $\boldsymbol{x}_i$ 的输出，$\boldsymbol{x}$ 是待分类的新测试样本。 核在计算函数 $\phi(\boldsymbol{x})$ 映射后数据点的点积时，实际上不用计算这个映射函数，这种方法被称作“核函数”：
$$
K(\boldsymbol{x},\boldsymbol{x}_i) = \boldsymbol{\varphi}(\boldsymbol{x})\cdot\boldsymbol{\varphi}(\boldsymbol{x}_i)
$$

#### 6.1 间隔和支持向量

在样本空间中，划分超平面可通过如下线性方程来描述：
$$
\boldsymbol{w}^\top\boldsymbol{x} + b = 0 \tag{6-1}
$$
其中 $\boldsymbol{w} = (w_1,…,w_d)$ 为法向量，决定了超平面的方向；$b$ 为位移项，决定了超平面与原点之间的距离。显然，划分超平面可被法向量 $\boldsymbol{w}$ 和位移 $b$ 确定。样本空间中任意点 $\boldsymbol{x}$ 到超平面的距离可写为：
$$
r = \frac{\vert\boldsymbol{w}^\top\boldsymbol{x}+b\vert}{\Vert\boldsymbol{w}\Vert} \tag{6-2}
$$
假设已标记的实例是线性可分的，这意味着存在一对 $(\boldsymbol{w},b)$ 使得：
$$
\begin{cases}
\boldsymbol{w}^\top\boldsymbol{x} + b \ge +1, &y = +1 \\
\boldsymbol{w}^\top\boldsymbol{x} + b \le -1,&y = -1
\end{cases} \tag{6-3}
$$
训练样本集中与超平面距离最近的训练样本点称为**支持向量**。如图 6-1 所示，支持向量使式 $\text{(6-3)}$ 的等号成立，即：

- 对于 $y=+1$ 的正例点，支持向量在超平面 $H_1:\boldsymbol{w}^\top\boldsymbol{x} + b = 1$ 上；
- 对于 $y=-1$ 的负例点，支持向量在超平面 $H_2:\boldsymbol{w}^\top\boldsymbol{x} + b = -1$ 上。

![6-1](6-1.jpeg)

<center>图 6-1 支持向量和间隔</center><br>

可以看到 $H_1$ 和 $H_2$ 平行，划分超平面与它们平行且位于它们中央。其中两个异类支持向量到超平面的距离之和，或者说 $H_1$ 与 $H_2$ 之间的距离称为**间隔**，为：
$$
\gamma
 = \frac{2}{\Vert \boldsymbol{w} \Vert} \tag{6-4}
$$
我们要找到具有“最大间隔”的划分超平面，也就是要找到能满足 $\text{(6-3)}$ 中约束的参数 $\boldsymbol{w}$ 和 $b$，使得 $\gamma$ 最大。于是这一问题可以形式化为：
$$
\begin{gather}
\max_{\boldsymbol{w},b} \frac{2}{\Vert \boldsymbol{w} \Vert}\\
\text{s.t. } y_i(\boldsymbol{w}^\top\boldsymbol{x}_i + b) \ge 1, \quad i=1,...,l
\end{gather} \tag{6-5}
$$
显然，为了最大化间隔，仅需最大化 $\frac{1}{\Vert \boldsymbol{w} \Vert}$，这等价于最小化 $\Vert \boldsymbol{w} \Vert^2$。于是，式 $\text{(6-5)}$ 可重写为：
$$
\begin{gather}
\min_{\boldsymbol{w},b} \frac{1}{2} \Vert \boldsymbol{w} \Vert^2\\
\text{s.t. } y_i(\boldsymbol{w}^\top\boldsymbol{x}_i + b) \ge 1, \quad i = 1,...,l 
\end{gather}\tag{6-6}
$$
这就是**支持向量机**的基本型。

> 1. 间隔貌似仅与 $\boldsymbol{w}$ 有关，但事实上 $b$ 通过约束隐式地影响着 $\boldsymbol{w}$ 的取值，进而对间隔产生影响。
> 2. 在决定划分超平面时只有支持向量起作用，而其他样本点并不起作用。如果移动支持向量将改变所求的解，而移动其他样本点，甚至去掉这些点，解都不会改变。由于支持向量在确定划分超平面中起着确定性作用，因此将这种分类模型称为支持向量机。

#### 6.2 学习算法

式 $\text{(6-6)}$ 是一个凸二次规划问题，能直接用现成的优化计算包求解，但是我们可以有更高效的办法。

对式 $\text{(6-6)}$ 使用拉格朗日乘子法可得到其“对偶问题”。具体来说，对 $\text{(6-6)}$ 的每条约束添加拉格朗日乘子 $\alpha_i \ge 0$，则该问题的拉格朗日函数可以写为：
$$
L(\boldsymbol{w},b,\boldsymbol{\alpha}) = \frac{1}{2}\Vert\boldsymbol{w}\Vert^2 + \sum_{i=1}^l \alpha_i\big(1-y_i(\boldsymbol{w}^\top\boldsymbol{x}_i+b)\big) \tag{6-7}
$$
根据拉格朗日对偶性，原始问题 $\text{(6-6)}$ 的对偶问题是：
$$
\max_{\boldsymbol{\alpha}} \min_{\boldsymbol{w},b} L(\boldsymbol{w},b,\boldsymbol{\alpha}) \tag{6-8}
$$
所以需要先求 $L(\boldsymbol{w},b,\boldsymbol{\alpha})$ 对 $\boldsymbol{w},b$ 的极小，再求对 $\boldsymbol{\alpha}$ 的极大。令 $L(\boldsymbol{w},b,\boldsymbol{\alpha})$ 对 $\boldsymbol{w}$ 和 $b$ 的偏导为零可得：
$$
\begin{align}
\nabla_{\boldsymbol{w}}L(\boldsymbol{w},b,\boldsymbol{\alpha}) &= \boldsymbol{w} - \sum_{i=1}^l\alpha_i y_i \boldsymbol{x}_i = 0 \\
\nabla_{b}L(\boldsymbol{w},b,\boldsymbol{\alpha}) &= \sum_{i=1}^l \alpha_i y_i = 0
\end{align}
$$

$$
\boldsymbol{w} = \sum_{i=1}^l \alpha_iy_i\boldsymbol{x}_i \tag{6-9}
$$

$$
0 = \sum_{i=1}^l\alpha_i y_i \tag{6-10}
$$

将 $\text{(6-9)}$ 代入 $\text{(6-7)}$，即可将 $L(\boldsymbol{w},b,\boldsymbol{\alpha})$ 中的 $\boldsymbol{w}$ 和 $b$ 消去，
$$
\begin{align}
L(\boldsymbol{w},b,\boldsymbol{\alpha}) &= \frac{1}{2} \sum_{i=1}^l\sum_{j=1}^l \alpha_i\alpha_j y_i y_j \boldsymbol{x}_i^\top \boldsymbol{x}_j + \sum_{i=1}^l\alpha_i - \sum_{i=1}^l \alpha_iy_i\bigg(\Big(\sum_{j=1}^l \alpha_j y_j \boldsymbol{x}_j\Big)\boldsymbol{x}_i +b\bigg)\\
&=\sum_{i=1}^l \alpha_i - \frac{1}{2}\sum_{i=1}^l\sum_{j=1}^l \alpha_i\alpha_j y_i y_j\boldsymbol{x}_i^\top\boldsymbol{x}_j
\end{align}
$$
再考虑式 $\text{(6-10)}$ 的约束，式 $\text{(6-8)}$ 就可以化为
$$
\begin{gather}
\max_{\boldsymbol{\alpha}} \sum_{i=1}^l \alpha_i -\frac{1}{2}\sum_{i=1}^l\sum_{j=1}^l \alpha_i\alpha_j y_i y_j \boldsymbol{x}_i^\top\boldsymbol{x}_j \\
\text{s.t. } \sum_{i=1}^l\alpha_i y_i = 0,\\
\alpha_i \ge 0,i=1,...,l
\end{gather} \tag{6-11}
$$
将式 $\text{(6-11)}$ 的目标函数由求极大转换成求极小，就得到下面与之等价的对偶最优化问题：
$$
\begin{gather}
\min_{\boldsymbol{\alpha}} \frac{1}{2}\sum_{i=1}^l\sum_{j=1}^l \alpha_i\alpha_j y_i y_j \boldsymbol{x}_i^\top\boldsymbol{x}_j - \sum_{i=1}^l \alpha_i\\
\text{s.t. } \sum_{i=1}^l\alpha_i y_i = 0,\\
\alpha_i \ge 0,i=1,...,l
\end{gather} \tag{6-12}
$$
解出 $\boldsymbol{\alpha}$ 后，求出 $\boldsymbol{w}$ 和 $b$ 即可得到模型
$$
\begin{align}
f(\boldsymbol{x}) &= \text{sign}(\boldsymbol{w}^\top\boldsymbol{x} + b)\\
&= \text{sign}\bigg(\sum_{i=1}^l\alpha_iy_i\boldsymbol{x}_i^\top\boldsymbol{x} + b\bigg)
\end{align} \tag{6-13}
$$
注意到式 $\text{(6-6)}$ 中有不等式约束，因此上述过程需要满足 KKT (Karush-Kuhn-Tucker) 条件，即要求：
$$
\begin{cases}
\alpha_i \ge 0\\
y_i(\boldsymbol{w}^\top\boldsymbol{x}_i + b) - 1 \ge 0\\
\alpha_i(y_i(\boldsymbol{w}^\top\boldsymbol{x}_i + b)-1) = 0
\end{cases} \tag{6-14}
$$
于是，对任意训练样本 $(\boldsymbol{x}_i, y_i)$，总有 $\alpha_i = 0$ 或 $y_i(\boldsymbol{w}^\top\boldsymbol{x}_i + b) = 1$。

- 若 $\alpha_i= 0$，则该样本将不会在式 $\text{(6-13)}$  的求和中出现，也就不会对 $f(\boldsymbol{x})$ 有任何影响；
- 若 $\alpha_i > 0$，则必有 $y_if(\boldsymbol{x}_i) = 1$，所对应的样本点位于最大间隔边界上，是一个支持向量。

因此训练完成后，大部分的训练样本都不需保留，最终模型仅与支持向量有关。最终分类是由加权的子分类 $y_i$ 的线性组合决定的，这些权重由输入样本和实例样本的标量积（当前样例和实例 $\boldsymbol{x}_i$ 之间相似性的度量）和参数 $\alpha_i$ 确定。 

式 $\text{(6-12)}$ 是一个二次规划问题，可使用通用的二次规划算法来求解；然而，该问题的规模正比于训练样本数，这会在实际任务中造成很大的开销。为了避开这个障碍，人们通过利用问题本身的特性，提出了很多高效的算法，SMO 就是其中一个著名的代表。

SMO 的基本思想是先固定 $\alpha_i$ 之外的所有参数，然后求 $\alpha_i$ 上的极值。由于存在约束 $\sum_{i=1}^l \alpha_iy_i= 0$，若固定 $\alpha_i$ 之外的其他变量，则 $\alpha_i$ 可由其他变量导出。于是，SMO 每次选择两个变量 $\alpha_i$ 和 $\alpha_j$，并固定其他参数。这样，在参数初始化后，SMO 不断执行如下两个步骤直至收敛：

- 选取一对需更新的变量 $\alpha_i$ 和 $\alpha_j$；
- 固定 $\alpha_i$ 和 $\alpha_j$ 以外的参数，求解式 $\text{(6-12)}$ 获得更新后的 $\alpha_i$ 和 $\alpha_j$。

注意到只需选取的 $\alpha_i$ 和 $\alpha_j$ 中有一个不满足 KKT 条件 $\text{(6-14)}$，目标函数就会在迭代后减小。直观来看，KKT 条件违背的程度越大，则变量更新后可能导致的目标函数值减幅越大。于是，SMO 先选取违背 KKT 条件程度最大的变量。第二个变量应选择一个使目标函数值减小最快的变量，但由于比较各变量所对应的目标函数值减幅的复杂度过高，因此 SMO 采用了一个启发式：使选取的两变量所对应样本之间的间隔最大。

> 一种直观的解释是，这样的两个变量有很大的差别，与对两个相似的变量进行更新相比，对它们进行更新会带给目标函数值更大的变化。

SMO 算法之所以高效，恰由于在固定其他参数后，仅优化两个参数的过程能做到非常高效。具体来说，仅考虑 $\alpha_i$ 和 $\alpha_j$ 时，式 $\text{(6-12)}$ 中的约束可重写为
$$
\alpha_iy_i + \alpha_jy_j = c, \quad\alpha_i\ge0,\alpha_j\ge0 \tag{6-15}
$$
其中 $c = -\sum_{k \ne i,j} \alpha_k y_k$ 是使 $\sum_{i=1}^l\alpha_iy_i = 0$ 成立的常数。用 $\alpha_iy_i + \alpha_jy_j = c$ 消去式 $\text{(6-12)}$ 中的变量 $\alpha_j$，则得到一个关于 $\alpha_i$ 的单变量二次规划问题，仅有的约束是 $\alpha_i \ge 0$。这样的二次规划问题具有闭式解，于是不必调用数值优化算法即可高效地计算出更新后的 $\alpha_i \ge 0$ 和 $\alpha_j \ge 0$。 

最后确定偏置项 $b$。注意到对任意支持向量 $(\boldsymbol{x}_s,y_s)$ 都有 $y_s(\boldsymbol{w}^\top\boldsymbol{x}_s+b)=1$，即
$$
y_s\Bigg(\sum_{i\in S}\alpha_iy_i\boldsymbol{x}_i^\top\boldsymbol{x}_s + b\Bigg)=1 \tag{6-16}
$$
其中 $S = \{i \mid \alpha_i > 0, i = 1,2,…,m\}$ 为所有支持向量的下标集。理论上，可选取任意支持向量并通过求解式 $\text{(6-16)}$ 获得 $b$，但现实任务中常采用一种更鲁棒的做法：使用所有支持向量求解的平均值。
$$
b = \frac{1}{\vert S\vert}\sum_{s \in S}\Bigg(y_s - \sum_{i \in S}\alpha_iy_i\boldsymbol{x}_i^\top\boldsymbol{x}_s\Bigg) \tag{6-17}
$$

#### 6.3 非线性假设

上述技术可以扩展到非线性分类器，这需要将输入数据 $\boldsymbol{x}$ 映射成高维特征向量 $\boldsymbol{\varphi}(\boldsymbol{x})$ 并在转化后的特征空间再使用线性分类。

> 如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本线性可分。

于是，在特征空间中划分超平面可以表示为：
$$
\boldsymbol{w}^\top\boldsymbol{\varphi}(\boldsymbol{x}) + b = 0 \tag{6-18}
$$
类似于式 $\text{(6-6)}$，有：
$$
\begin{gather}
\min_{\boldsymbol{w},b} \frac{1}{2} \Vert \boldsymbol{w} \Vert^2\\
\text{s.t. } y_i(\boldsymbol{w}^\top\boldsymbol{\varphi}(\boldsymbol{x}_i) + b) \ge 1, \quad i = 1,...,l 
\end{gather}\tag{6-19}
$$
其对偶问题是：
$$
\begin{gather}
\max_{\boldsymbol{\alpha}} \sum_{i=1}^l \alpha_i -\frac{1}{2}\sum_{i=1}^l\sum_{j=1}^l \alpha_i\alpha_j y_i y_j \boldsymbol{\varphi}(\boldsymbol{x}_i)^\top\boldsymbol{\varphi}(\boldsymbol{x}_j) \\
\text{s.t. } \sum_{i=1}^l\alpha_i y_i = 0,\\
\alpha_i \ge 0,i=1,...,l
\end{gather} \tag{6-20}
$$
式中的 $\boldsymbol{\varphi}(\boldsymbol{x}_i)^\top\boldsymbol{\varphi}(\boldsymbol{x}_j)$ 就是样本 $\boldsymbol{x}_i$ 与 $\boldsymbol{x}_j$ 映射到特征空间之后的内积。但是特征空间的维数可能很高，甚至可能是无穷维，因此直接计算 $\boldsymbol{\varphi}(\boldsymbol{x}_i)^\top\boldsymbol{\varphi}(\boldsymbol{x}_j)$ 通常是非常困难的。为了避开这个障碍，我们引入核函数：
$$
K(\boldsymbol{x},\boldsymbol{y}) = \boldsymbol{\varphi}(\boldsymbol{x})\cdot\boldsymbol{\varphi}(\boldsymbol{y})
$$
于是式 $\text{(6-20)}$ 可以重写为：
$$
\begin{gather}
\max_{\boldsymbol{\alpha}} \sum_{i=1}^l \alpha_i -\frac{1}{2}\sum_{i=1}^l\sum_{j=1}^l \alpha_i\alpha_j y_i y_j K(\boldsymbol{x}_i,\boldsymbol{x}_j) \\
\text{s.t. } \sum_{i=1}^l\alpha_i y_i = 0,\\
\alpha_i \ge 0,i=1,...,l
\end{gather} \tag{6-21}
$$
现在 SVM 分类器变为：
$$
\begin{align}
f(\boldsymbol{x}) &= \text{sign}\bigg(\sum_{i=1}^l\alpha_iy_i\boldsymbol{\varphi}(\boldsymbol{x})^\top\boldsymbol{\varphi}(\boldsymbol{x}_i) + b\bigg)\\
&=\text{sign}\bigg(\sum_{i=1}^l\alpha_iy_iK(\boldsymbol{x},\boldsymbol{x}_i)+b\bigg)
\end{align}\tag{6-22}
$$
我们希望样本在特征空间内线性可分，因此特征空间的好坏对支持向量机的性能至关重要。问题是，在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，而核函数也仅是隐式定义了这个特征空间。于是，“核函数选择”成为支持向量机的最大变数，若核函数选择不合适，意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳。表 6-1 列出了常用的核函数：

<br>

<center>表 6-1 常用的核函数</center>

$$
\begin{array}{c|c} 
\hline 
名称 & 表达式 & 参数\\
\hline 
线性核 & K(\boldsymbol{x},\boldsymbol{y})=\boldsymbol{x}^\top\boldsymbol{y} & \\
\hline
多项式核 & K(\boldsymbol{x},\boldsymbol{y})=(\boldsymbol{x}^\top\boldsymbol{y})^d & d\ge1 \text{ 为多项式的次数}\\
\hline
高斯核 & K(\boldsymbol{x},\boldsymbol{y})=\exp(-\frac{\Vert\boldsymbol{x}-\boldsymbol{y}\Vert^2}{2\sigma^2}) & \sigma\gt0 \text{ 为高斯核的带宽(width)}\\
\hline
拉普拉斯核 & K(\boldsymbol{x},\boldsymbol{y})=\exp(-\frac{\Vert\boldsymbol{x}-\boldsymbol{y}\Vert}{\sigma}) & \sigma\gt0\\
\hline
\text{sigmoid 核} & K(\boldsymbol{x},\boldsymbol{y})=\tanh(\beta\boldsymbol{x}^\top\boldsymbol{y}+\theta) & \tanh \text{ 为双曲正切函数，} \beta\gt0，\theta\lt0\\
\hline
\end{array}
$$

此外，还可通过函数组合得到新的核函数，例如：

- 若 $K_1$ 和 $K_2$ 为核函数，则对于任意正数 $\gamma_1$、$\gamma_1$，其线性组合 $\gamma_1K_1+\gamma_2K_2$ 也是核函数。
- 若 $K_1$ 和 $K_2$ 为核函数，则核函数的直积 $K_1\otimes K_2(\boldsymbol{x},\boldsymbol{y})=K_1(\boldsymbol{x},\boldsymbol{y})K_2(\boldsymbol{x},\boldsymbol{y})$ 也是核函数。
- 若 $K_1$ 为核函数，则对于任意函数 $g(\boldsymbol{x})$，$K(\boldsymbol{x},\boldsymbol{y})=g(\boldsymbol{x})K_1(\boldsymbol{x},\boldsymbol{y})g(\boldsymbol{y})$ 也是核函数。

> 在核函数的选择方面有一些基本的经验，例如对于文本数据通常采用线性核，情况不明时可先尝试高斯核。此外，核函数不是 SVM 专用的，只要能将目标函数能完全地用内积的形式来表示，都可使用核函数来高效地计算高维向量空间的分类结果。

#### 6.4 不可分问题和软间隔

我们一直假定训练样本在原始空间或者映射后的特征空间中是线性可分的，即存在一个超平面能将不同类的样本完全划分开。然而，在现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分；退一步说，即便恰好找到了某个核函数使得训练集在特征空间中线性可分，也很难断定这个貌似线性可分的结果不是由于过拟合所造成的。

例如，看下面两张图：

![6-2](6-2.jpg)

可以看到当数据集中存在离群点（可能是噪声）时会造成超平面的移动，间隔缩小，可见以前的模型对噪声非常敏感。再有甚者，如果离群点在另外一个类中，那么这时候就线性不可分了。

缓解该问题的一个办法是允许支持向量机在一些样本上出错。为此，要引入**软间隔**的概念。具体来说，前面介绍的支持向量机形式是要求所有样本均满足约束：
$$
y_i(\boldsymbol{w}^\top\boldsymbol{x}_i+b)\ge1
$$
即要求所有样本都必须划分正确，这称为**硬间隔**，而软间隔则是允许某些样本不满足这个约束。根据软间隔的思想，我们把最优化问题调整为：
$$
\begin{gather}
\min_{\gamma,\boldsymbol{w},b}\frac{1}{2}\Vert\boldsymbol{w}\Vert^2+C\sum_{i=1}^l\xi_i \\
\text{s.t. } y_i(\boldsymbol{w}^\top\boldsymbol{x}_i+b)\ge1-\xi_i\\
\xi_i\ge0,\quad i=1,\cdots,l
\end{gather}\tag{6-23}
$$
这就是常用的**软间隔支持向量机**。非负变量 $\xi_i$ 被称为“松弛变量”，引入 $\xi_i$ 后就允许某些样本点的函数间隔小于 1 甚至为负，即样本点在间隔区间之内甚至可以在对方的区域内。而放松限制条件后，我们需要重新调整目标函数，以对离群点进行处罚，因此在目标函数后面加上 $C\sum_{i=1}^l\xi_i$。这里的 $C$ 是离群点的权重， $C$ 越大表明离群点对目标函数影响越大，也就是越不希望看到离群点。

> 当 $C$ 无穷大时，式 $\text{(6-23)}$ 等价于式 $\text{(6-6)}$；当 $C$ 取有限值时，式 $\text{(6-23)}$ 允许一些样本不满足约束。

与之前的做法相似，我们也使用拉格朗日对偶性将最优化问题转换为对偶问题。首先，通过拉格朗日乘子法构造式 $\text{(6-23)}$ 的拉格朗日函数：
$$
\begin{align}L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\xi},\boldsymbol{\mu})&=\frac{1}{2}||\boldsymbol{w}||^2+C\sum_{i=1}^{l}\xi_i+\sum_{i=1}^l\alpha_i\big(1-\xi_i-y_i(\boldsymbol{w}^\top\boldsymbol{x}_i+b)\big)-\sum_{i=1}^l\mu_i\xi_i\end{align} \tag{6-24}
$$
其中 $\alpha_i\ge0$，$\mu_i\ge0$ 是拉格朗日乘子。根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：
$$
\max_{\boldsymbol{\alpha}}\min_{\boldsymbol{w},b,\boldsymbol{\xi}}L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\xi},\boldsymbol{\mu}) \tag{6-25}
$$
令 $L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\xi},\boldsymbol{\mu})$ 对 $\boldsymbol{w},b,\xi_i$ 的偏导为零可得：
$$
\begin{align}
\boldsymbol{w}&=\sum_{i=1}^l\alpha_iy_i\boldsymbol{x}_i \\
0 &= \sum_{i=1}^l\alpha_iy_i\\
C &= \alpha_i + \mu_i
\end{align} \tag{6-26}
$$
把 $\text{(6-26)}$ 代入 $\text{(6-24)}$，即得：
$$
\begin{align}L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\xi},\boldsymbol{\mu})&=\frac{1}{2}\boldsymbol{w}^\top\boldsymbol{w}+C\sum_{i=1}^l\xi_i+\sum_{i=1}^l\alpha_i-\sum_{i=1}^l\alpha_i\xi_i\\&\quad-\sum_{i=1}^l\alpha_iy_i(\boldsymbol{w}^\top\boldsymbol{x}_i)-\sum_{i=1}^l\alpha_iy_ib-\sum_{i=1}^l\mu_i\xi_i\end{align}
$$
因为 $\sum_{i=1}^l\alpha_iy_i=0$，所以 $\sum_{i=1}^l\alpha_iy_ib=0$；又因为 $C=\alpha_i+\mu_i$，所以
$$
C\sum_{i=1}^l\xi_i-\sum_{i=1}^l\alpha_i\xi_i-\sum_{i=1}^l\mu_i\xi_i=\sum_{i=1}^l(C-\alpha_i-\mu_i)\xi_i=0
$$
所以上式可以进一步简化为：
$$
\begin{align}L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\xi},\boldsymbol{\mu})&=\frac{1}{2}\boldsymbol{w}^\top\boldsymbol{w}+\sum_{i=1}^l\alpha_i-\sum_{i=1}^l\alpha_iy_i(\boldsymbol{w}^\top\boldsymbol{x}_i)\\&=\frac{1}{2}\boldsymbol{w}^\top\sum_{i=1}^l\alpha_iy_i\boldsymbol{x}_i-\boldsymbol{w}^\top\sum_{i=1}^l\alpha_iy_i\boldsymbol{x}_i+\sum_{i=1}^l\alpha_i\\&=-\frac{1}{2}\bigg(\sum_{i=1}^l\alpha_iy_i\boldsymbol{x}_i\bigg)^\top\sum_{i=1}^l\alpha_iy_i\boldsymbol{x}_i+\sum_{i=1}^l\alpha_i\\&=\sum_{i=1}^l\alpha_i-\frac{1}{2}\sum_{i=1}^l\sum_{j=1}^l\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i{}^\top\boldsymbol{x}_j\end{align}
$$
于是式 $\text{(6-25)}$ 可以进一步化为：
$$
\begin{gather}
\max_{\boldsymbol{\alpha}}\sum_{i=1}^l\alpha_i-\frac{1}{2}\sum_{i=1}^l\sum_{j=1}^l\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^\top\boldsymbol{x}_j\\
\text{s.t. } \sum_{i=1}^l\alpha_iy_i=0,\\
0\le\alpha_i\le C,\quad i=1,\cdots,l
\end{gather} \tag{6-27}
$$
将上式与硬间隔下的对偶问题对比可以看到，两者唯一的差别就在于对偶变量的约束不同，$\alpha_i$ 又多了 $\alpha_i\le C$ 的限制。于是，可采用第 6.2 节中同样地算法求解 $\text{(6-27)}$。同样地，对软间隔支持向量机的 KKT 条件也发生了变化：
$$
\begin{cases}\alpha_i\ge0,\quad\mu_i\ge0\\
y_i(\boldsymbol{w}^\top\boldsymbol{x}_i+b)-1+\xi_i\ge0\\
\alpha_i\Big(y_i(\boldsymbol{w}^\top\boldsymbol{x}_i+b)-1+\xi_i\Big)=0\\\xi_i\ge0,\quad\mu_i\xi_i=0\end{cases} \tag{6-28}
$$
于是，对任意训练样本 $(\boldsymbol{x}_i,y_i)$，总有 $\alpha_i=0$ 或 $y_i(\boldsymbol{w}^\top\boldsymbol{x}_i+b)=1-\xi_i$。

- 若 $\alpha_i=0$，则该样本不会对模型有任何影响；
- 若 $\alpha_i\gt0$，则必有 $y^{(i)}(\boldsymbol{w}^T\boldsymbol{x}^{(i)}+b)=1-\xi_i$，即该样本是支持向量：由 $C=\alpha_i+\mu_i$，若 $\alpha_i\lt C$，则 $\mu_i\gt0$，进而有 $\xi_i=0$，即该样本恰在最大间隔边界上；若 $\alpha_i=C$，则有 $\mu_i=0$，此时若 $\xi_i\le 1$ 则该样本落在最大间隔内部，若 $\xi_i\gt1$ 则该样本被错误分类。

由此可以看出，软间隔支持向量机的最终模型也仅与支持向量有关。

<div style="page-break-after: always;"></div>

